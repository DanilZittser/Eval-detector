#!./venv/bin/python3

from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Union

import click
import numpy as np
import pandas as pd
from nptyping import NDArray, Shape, Int32, Float32
from tabulate import tabulate


AbsoluteNDArrayBoxes = NDArray[Shape['*, [left, top, right, bottom]'], Int32]
RelativeNDArrayBoxes = NDArray[Shape['*, [left, top, right, bottom]'], Float32]
NDArrayBoxes = Union[AbsoluteNDArrayBoxes, RelativeNDArrayBoxes]


def compute_iou(boxes1: NDArrayBoxes, boxes2: NDArrayBoxes) -> NDArray[Shape['* num_boxes1, * num_boxes2'], Float32]:
    """Computes pairwise IOU matrix.
    Args:
        boxes1: matrix with shape `(M, 4)` where each row is of the format `[left, top, right, bottom]`
        boxes2: matrix with shape `(N, 4)` where each row is of the format `[left, top, right, bottom]`
    Returns:
        matrix with shape `(M, N)`
    """
    lu = np.maximum(boxes1[:, None, :2], boxes2[:, :2])
    rd = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])
    intersection = np.maximum(0.0, rd - lu)
    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]
    boxes1_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    boxes2_area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
    union_area = np.maximum(boxes1_area[:, None] + boxes2_area - intersection_area, 1e-32)
    return np.clip(intersection_area / union_area, 0.0, 1.0).astype(np.float32)


def read_detections_file(filepath: Path, is_ground_truth: bool) -> pd.DataFrame:
    """Reads detections file.
    Args:
        filepath: path to ground truth or detections file
        is_ground_truth: whether the file is ground truth or detections
    Returns:
        dataframe with columns [`image_name`, `left`, `top`, `right`, `bottom`, `label`, `score`]
    """
    columns: List[str] = ['image_name', 'box', 'label'] + (['score'] if not is_ground_truth else [])
    data: pd.DataFrame = pd.read_csv(filepath, sep=' ', header=None, names=columns)

    if is_ground_truth:
        data['score'] = 1.0

    data: pd.DataFrame = pd.merge(
        left=data,
        right=pd.DataFrame(data['box'].str.split(',').tolist(), columns=['left', 'top', 'right', 'bottom']).astype(int),
        left_index=True,
        right_index=True,
    )

    return data[['image_name', 'left', 'top', 'right', 'bottom', 'label', 'score']]


def get_metrics_by_label_as_dict(label: str, metrics: Dict[str, int]) -> Dict[str, str]:
    """Returns metrics for a given label as a dictionary.
    Args:
        label: label name
        metrics: dictionary with `true_positive`, `false_positive` and `false_negatives`keys
    Returns:
        dictionary with `precision`, `recall` and `f1` keys and corresponding values as strings
    """
    precision: float = metrics['true_positive'] / (metrics['true_positive'] + metrics['false_positive'])
    recall: float = metrics['true_positive'] / (metrics['true_positive'] + metrics['false_negative'])
    f1_score: float = 2 * (precision * recall) / (precision + recall)

    return {
        '': label,
        'precision': f'{precision:.2f}',
        'recall': f'{recall:.2f}',
        'f1_score': f'{f1_score:.2f}',
    }


@click.command()
@click.option('-g', type=click.Path(exists=True, dir_okay=False, path_type=Path), help='Path to ground truth file')
@click.option('-d', type=click.Path(exists=True, dir_okay=False, path_type=Path), help='Path to detection results file')
@click.option('-t', type=float, help='Overlap threshold')
def main(g: Path, d: Path, t: float):
    ground_truth: pd.DataFrame = read_detections_file(g, is_ground_truth=True)
    detection_results: pd.DataFrame = read_detections_file(d, is_ground_truth=False)

    acc_metrics: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    # Iterate over each ground truth objects on each image
    for (image_name, label), true_df in ground_truth.groupby(['image_name', 'label']):
        pred_df: pd.DataFrame = detection_results.query('image_name == @image_name and label == @label')

        acc_metrics[label]['total_true'] += len(true_df)
        acc_metrics[label]['total_pred'] += len(pred_df)
        # If there are no predictions for this label, then all ground truth objects are false negatives
        if (len(true_df) > 0) and (len(pred_df) == 0):
            acc_metrics[label]['false_negative'] += len(true_df)
            continue

        true_boxes: AbsoluteNDArrayBoxes = true_df[['left', 'top', 'right', 'bottom']].to_numpy()
        pred_boxes: AbsoluteNDArrayBoxes = pred_df[['left', 'top', 'right', 'bottom']].to_numpy()

        pairwise_iou: NDArray[Shape['* num_boxes1, * num_boxes2'], Float32] = compute_iou(
            boxes1=true_boxes,
            boxes2=pred_boxes,
        )

        used_rows, used_cols = set(), set()

        while True:
            # Find the best overlap
            argmax_row, argmax_col = np.unravel_index(indices=np.argmax(pairwise_iou), shape=pairwise_iou.shape)
            max_iou: float = pairwise_iou[argmax_row, argmax_col]
            # If best overlap is less than threshold, then there are no more matches
            if max_iou < t:
                break
            # Mark used rows and columns
            pairwise_iou[argmax_row, :] = float('-inf')
            pairwise_iou[:, argmax_col] = float('-inf')

            used_rows.add(argmax_row)
            used_cols.add(argmax_col)

            acc_metrics[label]['true_positive'] += 1
        # All remaining ground truth objects are false negatives
        acc_metrics[label]['false_negative'] += len(set(range(len(true_df))) - used_rows)
        # All remaining predictions are false positives
        acc_metrics[label]['false_positive'] += len(set(range(len(pred_df))) - used_cols)

    click.echo(
        tabulate(
            tabular_data=[get_metrics_by_label_as_dict(label, metrics) for label, metrics in acc_metrics.items()],
            headers='keys',
            tablefmt='psql',
        )
    )


if __name__ == '__main__':
    main()
